import os
from typing import Any

from dotenv import load_dotenv
from langchain.agents import AgentState
from langchain.agents.middleware import wrap_model_call, before_model, \
    SummarizationMiddleware, HumanInTheLoopMiddleware, ModelRequest
from langchain_community.chat_models import ChatZhipuAI
# ä¸æ”¯æŒbind_toolsçš„æ¨¡å‹
# from langchain_community.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI
from langchain_core.language_models import ModelProfile, BaseChatModel
from langchain_core.messages import HumanMessage, SystemMessage
from langgraph.runtime import Runtime

load_dotenv(override=True)


def build_model(model_stream: bool = False, enable_thinking=True,
                provider: str = 'openai') -> BaseChatModel:
    if provider == 'openrouter':
        return ChatOpenAI(
            openai_api_base="https://openrouter.ai/api/v1",
            model="openrouter/free",
            openai_api_key=os.getenv('OPENROUTER_API_KEY'),
            temperature=0.7,  # Add temperature parameter
            max_tokens=8192,  # Add max_tokens parameter
            streaming=model_stream,
            # æ·»åŠ è¶…æ—¶é…ç½®
            request_timeout=1200,  # æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries=3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
        )

    if provider == 'openai':
        return ChatOpenAI(
            openai_api_base="https://open.bigmodel.cn/api/paas/v4",
            model="glm-4.5-flash",
            openai_api_key=os.getenv('ZHIPUAI_API_KEY'),
            temperature=0.7,  # Add temperature parameter
            max_tokens=8192,  # Add max_tokens parameter
            streaming=model_stream,
            # æ·»åŠ è¶…æ—¶é…ç½®
            request_timeout=1200,  # æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries=3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
        )
    return ChatZhipuAI(
        model='glm-4.5-flash',
        zhipuai_api_key=os.getenv('ZHIPUAI_API_KEY'),
        temperature=0.7,  # Add temperature parameter
        max_tokens=8192,  # Add max_tokens parameter
        streaming=model_stream,
        profile=ModelProfile(reasoning_output=False),
        extra_body={
            "chat_template_kwargs": {
                "enable_thinking": enable_thinking
            }
        },
    )


@wrap_model_call
async def dynamic_model_routing(request: ModelRequest, handler):
    messages = request.messages
    last_user_msg = ''
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            if isinstance(msg.content, str):
                last_user_msg = msg.content
            elif isinstance(msg.content, list):
                last_user_msg = msg.content[0].get('text')
            else:
                raise Exception(f'æœªçŸ¥çš„msg.contentç±»å‹: {type(msg.content)}')
            break
    hard_keywords = ["è¯æ˜", "æ¨å¯¼", "ä¸¥è°¨", "chain of thought", "step-by-step", "reason step by step"]
    # èµ°å¼ºæ¨¡å‹æ¡ä»¶ï¼š å†å²æ¶ˆæ¯è¿‡é•¿ æœ€è¿‘ç”¨æˆ·è¾“å…¥å¾ˆé•¿ å‡ºç°å¤æ‚ä»»åŠ¡å…³é”®è¯
    is_hard = len(messages) > 10 or len(last_user_msg) > 120 or any(
        kw.lower() in last_user_msg for kw in hard_keywords)
    if not is_hard:
        request.model = build_model(model_stream=True, enable_thinking=False)
        print("é—®é¢˜ä¸éš¾ï¼Œæ¨¡å‹èµ°éæ€è€ƒæ¨¡å¼ï¼Œå‡å°‘å“åº”æ—¶é—´")
        print(request.model)
    # ğŸ‘‡asyncå¿…é¡»await
    return await handler(request)


@before_model
async def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any]:
    his_msg = state['messages']
    if len(his_msg) > 4:
        state['messages'] = his_msg[0:1] + his_msg[-3:]
    print()


def build_summarization_middleware() -> SummarizationMiddleware:
    return SummarizationMiddleware(
        model=build_model(model_stream=False, enable_thinking=False),
        max_tokens_before_summary=3000,
        messages_to_keep=10
    )


def build_human_in_the_loop_middleware():
    return HumanInTheLoopMiddleware(
        interrupt_on={
            "tavily_search": {
                "allowed_decisions": ['approve', 'edit', 'reject'],
                "description": lambda tool_call, state, runtime:
                f"æ¨¡å‹å‡†å¤‡è¿›è¡ŒTavilyæœç´¢ï¼š{state.get('query')}"
            },
            "get_weather": {
                "allowed_decisions": ["approve", "reject"]
            },
        },
        description_prefix='å·¥å…·æ‰§è¡Œéœ€è¦äººå·¥å®¡æ‰¹'
    )


summarizationMiddleware = build_summarization_middleware()
humanInTheLoopMiddleware = build_human_in_the_loop_middleware()

if __name__ == '__main__':
    model = build_model(model_stream=True, enable_thinking=True)
    messages = [
        SystemMessage(content="/no_think ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©ç†ã€‚"),
        HumanMessage(content="ä½ å¥½ï¼Œä½ ä¼šå•¥"),
    ]
    r = model.invoke(messages)
    print(r)
