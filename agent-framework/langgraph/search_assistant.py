from typing import TypedDict, Annotated, List, Dict
from langgraph.graph.message import add_messages
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatZhipuAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from tavily import TavilyClient

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver


class SearchState(TypedDict):
    messages: Annotated[list, add_messages]
    user_query: str  # ç»è¿‡LLMç†è§£åçš„ç”¨æˆ·éœ€æ±‚æ€»ç»“
    search_query: str  # ä¼˜åŒ–åç”¨äºTavily APIçš„æœç´¢æŸ¥è¯¢
    search_results: str  # Tavilyæœç´¢è¿”å›çš„ç»“æœ
    final_answer: str  # æœ€ç»ˆç”Ÿæˆçš„ç­”æ¡ˆ
    step: str  # æ ‡è®°å½“å‰æ­¥éª¤


# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# åˆå§‹åŒ–æ¨¡å‹
# æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ª llm å®ä¾‹æ¥é©±åŠ¨æ‰€æœ‰èŠ‚ç‚¹çš„æ™ºèƒ½
llm = ChatZhipuAI(
    model='glm-4-flash',
    api_key='62d5b9049126430f9255d00f7a72c91e.qa240op6bmKv3Axq',
    # base_url='https://open.bigmodel.cn/api/paas/v4',
    temperature=0.7
)


def llm_invoke(messages: List[Dict[str, str]]):
    from zhipuai import ZhipuAI

    client = ZhipuAI(api_key='62d5b9049126430f9255d00f7a72c91e.qa240op6bmKv3Axq')

    response = client.chat.completions.create(
        model="glm-4-flash",
        messages=messages,
        stream=False,
    )
    return response.choices[0].message.content


# åˆå§‹åŒ–Tavilyå®¢æˆ·ç«¯
tavily_client = TavilyClient(api_key='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')


def understand_query_node(state: SearchState) -> dict:
    """æ­¥éª¤1ï¼šç†è§£ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆæœç´¢å…³é”®è¯"""
    user_message = state["messages"][-1].content

    understand_prompt = f"""åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼š"{user_message}"
è¯·å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼š
1. ç®€æ´æ€»ç»“ç”¨æˆ·æƒ³è¦äº†è§£ä»€ä¹ˆ
2. ç”Ÿæˆæœ€é€‚åˆæœç´¢å¼•æ“çš„å…³é”®è¯ï¼ˆä¸­è‹±æ–‡å‡å¯ï¼Œè¦ç²¾å‡†ï¼‰

æ ¼å¼ï¼š
ç†è§£ï¼š[ç”¨æˆ·éœ€æ±‚æ€»ç»“]
æœç´¢è¯ï¼š[æœ€ä½³æœç´¢å…³é”®è¯]"""

    # response_text = llm.invoke([SystemMessage(content=understand_prompt)]).content
    response_text = llm_invoke([{"role": "user", "content": understand_prompt}])

    # è§£æLLMçš„è¾“å‡ºï¼Œæå–æœç´¢å…³é”®è¯
    search_query = user_message  # é»˜è®¤ä½¿ç”¨åŸå§‹æŸ¥è¯¢
    if "æœç´¢è¯ï¼š" in response_text:
        search_query = response_text.split("æœç´¢è¯ï¼š")[1].strip()

    return {
        "user_query": response_text,
        "search_query": search_query,
        "step": "understood",
        "messages": [AIMessage(content=f"æˆ‘å°†ä¸ºæ‚¨æœç´¢ï¼š{search_query}")]
    }


def tavily_search_node(state: SearchState) -> dict:
    """æ­¥éª¤2ï¼šä½¿ç”¨Tavily APIè¿›è¡ŒçœŸå®æœç´¢"""
    search_query = state["search_query"]
    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢: {search_query}")
        response = tavily_client.search(
            query=search_query, search_depth="basic", max_results=5, include_answer=True
        )
        # å¤„ç†å’Œæ ¼å¼åŒ–æœç´¢ç»“æœ
        res_list = [f"{i + 1}ã€{it['title']}\n\t{it['content']}" for i, it in enumerate(response.get('results', []))]
        search_results = '\n'.join(res_list)

        return {
            "search_results": search_results,
            "step": "searched",
            "messages": [AIMessage(content="âœ… æœç´¢å®Œæˆï¼æ­£åœ¨æ•´ç†ç­”æ¡ˆ...")]
        }
    except Exception as e:
        # å¤„ç†é”™è¯¯
        return {
            "search_results": f"æœç´¢å¤±è´¥ï¼š{e}",
            "step": "search_failed",
            "messages": [AIMessage(content="âŒ æœç´¢é‡åˆ°é—®é¢˜...")]
        }


def generate_answer_node(state: SearchState) -> dict:
    """æ­¥éª¤3ï¼šåŸºäºæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    if state["step"] == "search_failed":
        # å¦‚æœæœç´¢å¤±è´¥ï¼Œæ‰§è¡Œå›é€€ç­–ç•¥ï¼ŒåŸºäºLLMè‡ªèº«çŸ¥è¯†å›ç­”
        fallback_prompt = f"æœç´¢APIæš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·åŸºäºæ‚¨çš„çŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š\nç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}"
        # response = llm.invoke([SystemMessage(content=fallback_prompt)]).content
        response = llm_invoke([{"role": "user", "content": fallback_prompt}])
    else:
        # æœç´¢æˆåŠŸï¼ŒåŸºäºæœç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
        answer_prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœä¸ºç”¨æˆ·æä¾›å®Œæ•´ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š
ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}
æœç´¢ç»“æœï¼š\n{state['search_results']}
è¯·ç»¼åˆæœç´¢ç»“æœï¼Œæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”..."""
        # response = llm.invoke([SystemMessage(content=answer_prompt)]).content
        response = llm_invoke([{"role": "user", "content": answer_prompt}])

    return {
        "final_answer": response,
        "step": "completed",
        "messages": [AIMessage(content=response)]
    }


def create_search_assistant():
    workflow = StateGraph(SearchState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("understand", understand_query_node)
    workflow.add_node("search", tavily_search_node)
    workflow.add_node("answer", generate_answer_node)

    # è®¾ç½®çº¿æ€§æµç¨‹
    workflow.add_edge(START, "understand")
    workflow.add_edge("understand", "search")
    workflow.add_edge("search", "answer")
    workflow.add_edge("answer", END)

    # ç¼–è¯‘å›¾
    memory = InMemorySaver()
    app = workflow.compile(checkpointer=memory)
    return app


if __name__ == '__main__':
    assistant = create_search_assistant()
    input_state = {'messages': [HumanMessage(content="æ˜å¤©æˆ‘è¦å»åŒ—äº¬ï¼Œå¤©æ°”æ€ä¹ˆæ ·ï¼Ÿæœ‰åˆé€‚çš„æ™¯ç‚¹å—")]}
    config = {"configurable": {"thread_id": "test_thread"}}
    res = assistant.invoke(input=input_state, config=config)
    print(res['messages'][-1].content)
